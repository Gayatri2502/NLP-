{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG6SbjpDkeqsuiDlLDg0zv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gayatri2502/NLP-/blob/main/N_gram_model_to_predict_the_next_word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n64Wfh-7VffF",
        "outputId": "02e6da39-4df4-488b-e808-e706de0c6dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fox\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "from collections import defaultdict\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Define the order of the N-gram model\n",
        "n = 2\n",
        "\n",
        "# Generate the N-grams\n",
        "words = sentence.split()\n",
        "ngrams = ngrams(words, n)\n",
        "\n",
        "# Create a dictionary to store the frequency of each N-gram\n",
        "freq_dict = defaultdict(int)\n",
        "for gram in ngrams:\n",
        "    freq_dict[gram] += 1\n",
        "\n",
        "# Define a function to predict the next word based on the input sentence\n",
        "def predict_next_word(sentence, freq_dict, n):\n",
        "    # Split the input sentence into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Check if there are enough words in the sentence to generate an N-gram\n",
        "    if len(words) < n:\n",
        "        return None\n",
        "\n",
        "    # Extract the last n-1 words from the sentence to use as the prefix of the N-gram\n",
        "    prefix = tuple(words[-(n-1):])\n",
        "\n",
        "    # Find all N-grams with the same prefix\n",
        "    matching_ngrams = [gram for gram in freq_dict.keys() if gram[:-1] == prefix]\n",
        "\n",
        "    # If there are no matching N-grams, return None\n",
        "    if len(matching_ngrams) == 0:\n",
        "        return None\n",
        "\n",
        "    # Select the N-gram with the highest frequency\n",
        "    next_gram = max(matching_ngrams, key=freq_dict.get)\n",
        "\n",
        "    # Return the last word of the selected N-gram\n",
        "    return next_gram[-1]\n",
        "\n",
        "# Example usage\n",
        "next_word = predict_next_word(\"the quick brown\", freq_dict, n)\n",
        "print(next_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# This function calculates the freq of the (i+1)th\n",
        "# word in the whole corpus, where i is the index of\n",
        "# the sentence or the word.\n",
        "\n",
        "def next_word_freq(array, sentence):\n",
        "\t\n",
        "\tsen_len, word_list = len(sentence.split()), []\n",
        "\t\n",
        "\tfor i in range(len(array)):\n",
        "\n",
        "\t\t# If the sentence matches the sentence in the range (i, i+x)\n",
        "\t\t# and the length is less than the length of the corpus, append\n",
        "\t\t# the word to word_list.\n",
        "\t\t\n",
        "\t\tif ' '.join(array[i : i + sen_len]).lower() == sentence.lower():\n",
        "\n",
        "\t\t\tif i + sen_len < len(array) - 1:\n",
        "\n",
        "\t\t\t\tword_list.append(array[i + sen_len])\n",
        "\n",
        "\t# Return the count of each word in word_list\n",
        "\t\n",
        "\treturn dict(Counter(word_list))\n",
        "\n",
        "# Calculate the CDF of each word in the\n",
        "# Counter dictionary.\n",
        "\n",
        "def CDF(d):\n",
        "\t\n",
        "\tprob_sum, sum_vals = 0, sum(d.values())\n",
        "\t\n",
        "\tfor k, v in d.items():\n",
        "\n",
        "\t\t# Calculate the PMF of each word by dividing\n",
        "\t\t# the freq. by total of all frequencies then add\n",
        "\t\t# all the PMFs till ith word which is the CDF of\n",
        "\t\t# the ith word.\n",
        "\t\t\n",
        "\t\tpmf = v / sum_vals\n",
        "\t\tprob_sum += pmf\n",
        "\t\td[k] = prob_sum\n",
        "\n",
        "\t# Return cdf dictionary\n",
        "\t\n",
        "\treturn d\n",
        "\n",
        "# The main function reads the sentence/word as input\n",
        "# from user and reads the corpus file. For faster processing,\n",
        "# we have taken only the first 1000 words.\n",
        "\n",
        "\n",
        "def main(sent, x, n):\n",
        "\n",
        "\t# I am using this sample text here to illustrate the output.\n",
        "\t# If anyone wants to use a text file, he can use the same. The code\n",
        "\t# to read corpus from file has been commented below.\n",
        "\n",
        "\t# corpus = open('a.txt','r').read()\n",
        "\n",
        "\tcorpus = '''text The chance is unlikely if not done programmatically.\n",
        "\tHowever, imagine the game spawning multiple players at a spawn point,\n",
        "\tthis would be the exact same location. I'm not quite sure what you\n",
        "\tmean with spin,\t what does the integer reflect? Why is it a\n",
        "\tmismatch between data and structure? The structure does not\n",
        "\tassume a set amount of objects, it can be anything, that's why new\n",
        "\tnodes are created. It simply makes sure that there are not more than\n",
        "\tX leafs inside 1 node. The random is no option of course.\n",
        "\tMy splitting algorithm always created the maximum amount of nodes\n",
        "\talready, split over the current node. But I guess I have to change\n",
        "\tthis behaviour? Actually, all the books have different authors. And\n",
        "\tmost have a different location too. There will be some with the same\n",
        "\tlocation, but different authors, though. I think my library should be\n",
        "\table to store books with the same position. There are never\n",
        "\tequally-attractive leaf nodes. If a node is split, all childs will\n",
        "\treflect a different part of the parent node.'''\n",
        "\t\n",
        "\tl = corpus.split()\n",
        "\n",
        "\t# \"temp_out\" will be used to store each partial sentence\n",
        "\t# which will later be stored into \"sent\". \"out\" is used to store\n",
        "\t# the final output.\n",
        "\t\n",
        "\ttemp_out = ''\n",
        "\tout = sent + ' '\n",
        "\t\n",
        "\tfor i in range(n - x):\n",
        "\n",
        "\t\t# calling the next_word_freq method that returns\n",
        "\t\t# the frequency of each word next to sent in the\n",
        "\t\t# whole word corpus.\n",
        "\t\t\n",
        "\t\tfunc_out = next_word_freq(l, sent)\n",
        "\n",
        "\t\t# cdf_dict stores the cdf of each word in the above map\n",
        "\t\t# that is calculated using method CDF.\n",
        "\t\t\n",
        "\t\tcdf_dict = CDF(func_out)\n",
        "\t\t\n",
        "\t\t# We use a random number to predict the next word.\n",
        "\t\t# The word having its CDF greater than or equal to rand\n",
        "\t\t# and less than or equal to 1.\n",
        "\t\t\n",
        "\t\trand = random.uniform(0, 1)\n",
        "\n",
        "\t\t# If cdf_dict is empty, it means the word.sentence entered by you\n",
        "\t\t# does not exist in the corpus. Hence, break the loop and just print\n",
        "\t\t# the word entered by you. To implement this we use try-except block.\n",
        "\t\t# If an error occurs it implies there aren't enough values to unpack\n",
        "\t\t# and this can happen only when your input is absent from the corpus.\n",
        "\t\t\n",
        "\t\ttry: key, val = zip(*cdf_dict.items())\n",
        "\t\texcept: break\n",
        "\n",
        "\t\t# Iterate through the cdf values and find the smallest value\n",
        "\t\t# greater than or equal to the random number. That value is the\n",
        "\t\t# cdf of your predicted word. Add the key of the value to the output\n",
        "\t\t# string and update the \"sent\" variable as \"temp_out\".\n",
        "\t\t\n",
        "\t\tfor j in range(len(val)):\n",
        "\t\t\t\n",
        "\t\t\tif rand <= val[j]:\n",
        "\t\t\t\tpos = j\n",
        "\t\t\t\tbreak\n",
        "\t\t\t\t\t\n",
        "\t\ttemp_out = key[pos]\n",
        "\t\tout = out + temp_out + ' '\n",
        "\t\tsent = temp_out\n",
        "\t\t\n",
        "\tprint(out, end = '\\n\\n')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tinp_sent = 'is'\n",
        "\t# The output will have 10 words, including the input sentence/word.\n",
        "\tmain(inp_sent, len(inp_sent), 10)\n",
        "\n",
        "# Code contributed by Gagan Talreja.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBPOBQCWY0vU",
        "outputId": "d50bf8d3-fd67-4f99-9dd7-4f82e953ba64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is unlikely if a mismatch between data and structure? \n",
            "\n"
          ]
        }
      ]
    }
  ]
}