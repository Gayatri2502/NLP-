{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU4olfzCb1h5dM+2D+lP1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gayatri2502/NLP-Practise-/blob/main/Stop_words_and_tokenizer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLzszoag6-gj",
        "outputId": "747cf115-4e34-4c41-a9e7-37715bf743d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "\t\t\t\tshowing off the stop words filtration.\"\"\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "# converts the words in word_tokens to lower case and then checks whether\n",
        "#they are present in stop_words or not\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "#with no lower case conversion\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6IRZ_xf71Dd",
        "outputId": "bba7dce5-5f50-4551-dbe3-2ed8e8530543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking a text or a sentence into individual words, phrases, or other meaningful elements. These individual elements are known as tokens. Tokenization is a crucial step in natural language processing, as it allows the computer to process and analyze text data.\n",
        "\n",
        "For example, given the sentence: \"The quick brown fox jumps over the lazy dog\", tokenization would result in the following tokens: \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\".\n",
        "\n",
        "Stop words are a list of words that are commonly used in a language but do not convey significant meaning and are usually removed from text data during preprocessing. Examples of stop words in English include \"the\", \"a\", \"an\", \"and\", \"or\", \"in\", \"on\", \"at\", \"to\", \"of\", etc.\n",
        "\n",
        "Stop words removal is the process of removing these common words from text data before analysis. The removal of stop words can reduce the amount of noise or irrelevant information in the data and improve the accuracy of natural language processing algorithms.\n",
        "\n",
        "For example, the sentence \"The quick brown fox jumps over the lazy dog\" would be processed with stop words removal to become \"quick brown fox jumps lazy dog\".\n",
        "\n",
        "In natural language processing, tokenization and stop words removal are often used together to preprocess text data for further analysis. The tokenization step breaks down the text data into individual elements or tokens, and the stop words removal step removes the commonly occurring but insignificant words to reduce noise and improve the accuracy of the analysis."
      ],
      "metadata": {
        "id": "4LOXXFLX8gv8"
      }
    }
  ]
}